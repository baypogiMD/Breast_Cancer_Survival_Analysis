'''
# 06 â€“ Model Interpretability

## Purpose
This notebook explains **why** the predictive model makes its decisions.

We focus on:
- Global feature importance
- Local (patient-level) explanations
- Clinical plausibility of model behavior

Interpretability is essential for **clinical trust, auditability, and decision support**.
'''

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

pd.set_option("display.max_columns", None)

# Load feature-enhanced dataset
df = pd.read_csv("../data/processed/breast_cancer_features.csv")

df.head()

features = [
    "age",
    "tumor_size",
    "grade_ord",
    "node_ratio",
    "high_risk",
    "advanced_disease"
]

X = df[features]
y = df["event"]

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=6,
    random_state=42,
    class_weight="balanced"
)

rf.fit(X_train, y_train)

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_train)

shap.summary_plot(
    shap_values[1],
    X_train,
    plot_type="bar",
    show=True
)

shap.summary_plot(
    shap_values[1],
    X_train,
    show=True
)

patient_idx = 0

shap.force_plot(
    explainer.expected_value[1],
    shap_values[1][patient_idx],
    X_train.iloc[patient_idx],
    matplotlib=True
)

